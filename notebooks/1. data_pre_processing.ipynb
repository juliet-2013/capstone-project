{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.sprinklr.com%2Fblog%2Fchatbot-examples%2F&psig=AOvVaw3GjLwPVFaNAUG6e4xKJYH2&ust=1705391165437000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCJDLi8yZ34MDFQAAAAAdAAAAABAI\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display; display(Image(url=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.sprinklr.com%2Fblog%2Fchatbot-examples%2F&psig=AOvVaw3GjLwPVFaNAUG6e4xKJYH2&ust=1705391165437000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCJDLi8yZ34MDFQAAAAAdAAAAABAI\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color:white;display:fill;border-radius:8px;background-color:##800080;font-size:150%; letter-spacing:1.0px\"><p style=\"padding: 15px;color:white;\"><b><b><span style='color:white'><span style='color:#F1A424'> | </span> </span></b>Defining the Question</b></p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><span style='color:#F1A424'>|</span> Executive Summary:</b> \n",
    "\n",
    "**Mental health, fundamentally a state of well-being, is crucial for individuals to realize their abilities, manage life's normal stresses, work productively, and contribute to their communities. Despite the rising global prevalence of mental health issues, including a 13% increase over the last decade noted by the WHO, access to effective treatments remains uneven, particularly among urban youths who face distinct challenges and stressors.\n",
    " Saidika, a burgeoning mental health service provider for urban youth, has encountered challenges due to the growing demand for mental health services. The volume of clients has impeded the prompt allocation of therapy resources, particularly for urgent cases, prompting the need for innovative solutions to enhance the efficiency and effectiveness of mental health care delivery. By leveraging the capabilities of AI and advancements in NLP, the project aims to bridge the gap between the growing demand for mental health services and the current limitations in supply and accessibility.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><span style='color:#F1A424'>|</span> Problem Statement:</b> \n",
    "\n",
    "**Saidika's platform is currently unable to efficiently handle the increasing influx of clients seeking mental health services. The inability to quickly triage and prioritize client needs is leading to potential delays in addressing urgent cases, which could have severe consequences on the well-being of individuals in need.**\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><span style='color:#F1A424'>|</span> Proposed Solution:</b> \n",
    "\n",
    "**Main Objective is to integrate ban advanced AI-powered mental health chatbot into Saidika's existing platform\n",
    "to optimize client management processes, ensuring timely and appropriate allocation of therapy resources to those in need.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><span style='color:#F1A424'>|</span>Specific Obectives:</b> \n",
    "- **Client Categorization: To develop a chatbot that can accurately categorize clients based on their responses, distinguishing between varying levels of care requirements and scheduling clients based on their assessed needs and therapists' availability, optimizing the use of Saidika's resources.**\n",
    "- **Urgency Escalation: To ensure the chatbot is capable of rapidly identifying and escalating urgent cases to therapists, facilitating prompt intervention.**\n",
    "- **Service Accessibility: To broaden access to mental health care by providing a 24/7 chatbot service that will offer real-time interaction to clients who require immediate attention or a platform to express their concerns, bridging the gap until a professional is available.**\n",
    "- **Resource Optimization: To aid therapists in managing their workload more effectively by allowing the chatbot to handle routine inquiries and non-urgent interactions.**\n",
    "- **Data Collection and Analysis: To gather and analyze interaction data to continually improve the chatbot’s performance and the platform’s services.**\n",
    "- **User Experience Enhancement: To create a user-friendly chatbot interface that provides a supportive environment for clients to express their concerns.**\n",
    "- **Integration and Compatibility: To seamlessly integrate the chatbot into both web and mobile applications, ensuring functionality across various devices.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b><span style='color:#F1A424'>|</span> Project Impact:</b> \n",
    "\n",
    "**The successful implementation of the mental health chatbot is expected to significantly improve the scalability of Saidika's services, enabling them to handle a greater volume of clients without sacrificing the quality of care. This technological solution aims to not only streamline operations but also to provide a critical early support system for individuals seeking mental health assistance. The chatbot's ability to analyze data will also furnish Saidika with valuable insights, driving policy and decision-making to better serve the community's mental health needs. Ultimately, the project endeavors to foster a more resilient urban youth population, better equipped to contribute positively to their communities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color:white;display:fill;border-radius:8px;background-color:#800080;font-size:150%; letter-spacing:1.0px\"><p style=\"padding: 12px;color:white;\"><b><b><span style='color:white'><span style='color:#F1A424'>|</span></span></b>DATA PERTINENCE AND ATTRIBUTION</b></p></div>\n",
    "\n",
    "\n",
    "\n",
    "**The business aims to gain valuable insights into mental health trends, sentiments, and urgency levels by leveraging a diverse dataset acquired from public domain resources and Saidika's private, anonymized user data with proper consent and privacy law adherence. The data primarily consists of information gathered from health forums, Reddit, a dedicated mental health forum, and Beyond Blue.**\n",
    "\n",
    "**Data Preparation:**\n",
    "\n",
    "**Data Sources: Public domain resources and private Saidika user data.**\n",
    "\n",
    "**Variable Types:**\n",
    "\n",
    "- **Categorical variables: Representing various types of mental health issues.**\n",
    "\n",
    "- **Binary variables: Indicating urgency levels.**\n",
    "- **Continuous variables: Expressing sentiment scores associated with mental health discussions.**\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "\n",
    "- **Text data cleaning: Removal of identifiable information.**\n",
    "\n",
    "- **Tokenization: Breaking down text into tokens.**\n",
    "\n",
    "- **Lemmatization: Reducing words to their base or root form.**\n",
    "\n",
    "- **Vectorization: Converting text into numerical vectors suitable for Natural Language Processing (NLP) tasks.**\n",
    "\n",
    "**Libraries Used:**\n",
    "\n",
    "- **BeautifulSoup: Utilized for parsing and extracting data from HTML content.**\n",
    "\n",
    "- **Python Libraries (NLTK, spaCy): Applied for NLP tasks such as tokenization, lemmatization, and other text processing operations.**\n",
    "\n",
    "**Algorithms:**\n",
    "\n",
    "- **Logistic Regression: Employed for analyzing categorical and binary variables, predicting urgency levels based on mental health issues.**\n",
    "\n",
    "- **LSTM (Long Short-Term Memory): Utilized for sequence modeling in NLP, capturing dependencies in sentiment scores over the course of discussions.**\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers): Implemented for advanced contextualized embeddings, enhancing understanding of the nuanced context within mental health discourse.**\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer): Employed for generating human-like text responses and comprehending the context of mental health discussions.**\n",
    "\n",
    "**Overall, the objective is to extract meaningful insights, patterns, and correlations from this rich dataset, contributing to a deeper understanding of mental health issues, sentiments, and urgency levels, ultimately informing strategies for better mental health support and intervention.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color:white;display:fill;border-radius:8px;background-color:#800080;font-size:150%; letter-spacing:1.0px\"><p style=\"padding: 12px;color:white;\"><b><b><span style='color:white'><span style='color:#F1A424'>1 |</span></span></b>Data Loading & Preparation</b></p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>1.1 <span style='color:#F1A424'>|</span> Importing Necessary Libraries</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  #plotting statistical graphs\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import squarify\n",
    "from collections import Counter\n",
    "\n",
    "# Load the Text Cleaning Package\n",
    "import neattext.functions as nfx\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator ##is a data visualization technique used\n",
    "#for representing text data in which the size of each word indicates its frequency\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.linear_model import RidgeClassifier,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm ##new progress bars repeatedly\n",
    "import os\n",
    "import nltk ##building Python programs to work with human language data\n",
    "#import spacy #for training the NER model tokenize words\n",
    "#import random\n",
    "#from spacy.util import compounding\n",
    "#from spacy.util import minibatch\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "pd.set_option('use_mathjax', False)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>1.2 <span style='color:#F1A424'>|</span>Loading in our Data</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reddit_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c79e14100b1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load the dataset just using specific features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reddit_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda4\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda4\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda4\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda4\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda4\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_data.csv'"
     ]
    }
   ],
   "source": [
    "# load the dataset -> feature extraction -> data visualization -> data cleaning -> train test split\n",
    "# -> model building -> model training -> model evaluation -> model saving -> streamlit application deploy\n",
    "\n",
    "# load the dataset just using specific features\n",
    "df = pd.read_csv('reddit_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color:white;display:fill;border-radius:8px;background-color:#800080;font-size:150%; letter-spacing:1.0px\"><p style=\"padding: 12px;color:white;\"><b><b><span style='color:white'><span style='color:#F1A424'>2 |</span></span></b> Data Quality Checks</b></p></div>\n",
    "   \n",
    "- **Another crucial step in any project involves ensuring the quality of your data. Remember that your model’s performance is directly tied to the data it processes. Therefore, take the time to remove duplicates and handle missing values appropriately.**\n",
    "\n",
    "- **Here we always check for missing values, outliers and remove any unnecessary variables/features/columns. Since we have text data, outliers cannot be checked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>2.1 <span style='color:#F1A424'>|</span> Checking for NaN Values</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As noted earlier, we don not have any null values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>2.2 <span style='color:#F1A424'>|</span> Checking for Sentence Length Consistency</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This can give you an overview of the number of words per tweet. We also notice that some consist of less then five words hence won't be instrumental in constructing our predictive model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['tweet'].apply(len) > 5) , sum(df['tweet'].apply(len) <= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have `43464` tweets with characters gretaer than 5 and only `14` tweets with characters less than 5 characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset before filtering:\")\n",
    "print(df.shape)\n",
    "print(\"*\"*40)\n",
    "df = df[df['tweet'].apply(len) > 5]\n",
    "print(\"Shape of the dataset after filtering:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>2.3 <span style='color:#F1A424'>|</span> Checking for Duplicates</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())\n",
    "print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will have to check if indeed these are duplicate values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the duolicate values are indeed duplicates\n",
    "df[df.duplicated(subset=['tweet'],keep=False)].sort_values(by='tweet').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['id'] == 1094014959636410368]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the dataset does indeed contain entries that are duplicate tweets. We will go ahead and drop these duplicate entries although the number of duolicates `21772` accounts for almost half of our data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df.duplicated().sum())\n",
    "print(\"*\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"color:white;display:fill;border-radius:8px;background-color:#800080;font-size:150%; letter-spacing:1.0px\"><p style=\"padding: 12px;color:white;\"><b><b><span style='color:white'><span style='color:#F1A424'>3 |</span></span></b> Data Preprocessing</b></p></div>\n",
    "\n",
    "- **Preprocessing procedures are tokenizing(spliting),stemming and lemmatization which are dependent on the model you choose to use.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>3.1 <span style='color:#F1A424'>|</span> Text Cleaning (Source Text)</b> \n",
    "+ Mentions / User handles\n",
    "+ Hashtags\n",
    "+ URLs\n",
    "+ Special Characters\n",
    "+ Whitespaces\n",
    "+ Emojis\n",
    "+ Contractions\n",
    "+ Stopwords\n",
    "\n",
    "**For cleaning our text we will be using the NeatText Library. NeatText is a simple NLP package for cleaning textual data and text preprocessing. It offers a variety of features for cleaning unstructured text data, reducing noise (such as special characters and stopwords), and extracting specific information from the text. It can be used via an object-oriented approach or a functional/method-oriented approach, providing flexibility in its usage. The package includes classes such as TextCleaner, TextExtractor, and TextMetrics for different text processing tasks.**\n",
    "\n",
    "https://pypi.org/project/neattext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text cleaning packages\n",
    "\n",
    "import neattext as nt\n",
    "import neattext.functions as nfx\n",
    "\n",
    "# Methods and Attributes of the function\n",
    "dir(nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.1 <span style='color:#F1A424'>|</span> Mentions / User Handles</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise scan\n",
    "df['tweet'].apply(lambda x: nt.TextFrame(x).noise_scan()['text_noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all entries in 'tweet' column are strings\n",
    "df['tweet'] = df['tweet'].astype(str)\n",
    "\n",
    "# Now apply the clean_text function\n",
    "df['clean_tweet'] = df['tweet'].apply(lambda x: nfx.clean_text(x, puncts=False, stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract userhandles into another column before removing them\n",
    "df['userhandle'] = df['clean_tweet'].apply(nfx.extract_userhandles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the userhandles\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_userhandles)\n",
    "\n",
    "df[['tweet', 'clean_tweet', 'userhandle']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.2 <span style='color:#F1A424'>|</span> Hashtags</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags into another column before removing them\n",
    "df['hashtags'] = df['clean_tweet'].apply(nfx.extract_hashtags)\n",
    "\n",
    "df[['tweet', 'clean_tweet', 'hashtags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hashtags\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_hashtags)\n",
    "\n",
    "df[['tweet', 'clean_tweet', 'hashtags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.3 <span style='color:#F1A424'>|</span> URLs</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs into another column before removing them\n",
    "# If we were to remove the URLs after remove the special characters e.g '//' the function would be ubable to detect the URLs\n",
    "df['urls'] = df['clean_tweet'].apply(nfx.extract_urls)\n",
    "\n",
    "df[['tweet', 'clean_tweet', 'urls']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tweet', 'clean_tweet', 'urls']].loc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tweet', 'clean_tweet', 'urls']].loc[16515]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tweet', 'clean_tweet', 'urls']].loc[12827]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLS\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tweet', 'clean_tweet', 'urls']].loc[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.4 <span style='color:#F1A424'>|</span> Special Characters</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_special_characters)\n",
    "\n",
    "df[['tweet', 'clean_tweet']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.5 <span style='color:#F1A424'>|</span> Multiple Whitespaces</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespaces\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_multiple_spaces)\n",
    "\n",
    "df[['tweet', 'clean_tweet']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.6 <span style='color:#F1A424'>|</span> Emojis</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_emojis)\n",
    "\n",
    "df[['tweet', 'clean_tweet']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.7 <span style='color:#F1A424'>|</span> Contractions</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Apply the contractions.fix function to the clean_tweet column\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(contractions.fix)\n",
    "\n",
    "df[['tweet', 'clean_tweet']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.1.8 <span style='color:#F1A424'>|</span> Stopwords</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stopwords\n",
    "df['clean_tweet'].apply(lambda x: nt.TextExtractor(x).extract_stopwords())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stop words\n",
    "\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df[['tweet', 'clean_tweet']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Scan after cleaning text\n",
    "df['clean_tweet'].apply(lambda x: nt.TextFrame(x).noise_scan()['text_noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>3.2 <span style='color:#F1A424'>|</span> Linguistic Processing (Clean Text)</b> \n",
    "\n",
    "+ Tokenization\n",
    "+ Stemming / Lemmatization\n",
    "+ Parts of Speech Tagging\n",
    "+ Calculating Sentiment Based on Polarity & Subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.2.1 <span style='color:#F1A424'>|</span> Tokenization</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = df['clean_tweet'].loc[12827]\n",
    "\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "\n",
    "tokenizer.tokenize(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the clean_tweet column\n",
    "df['preprocessed_tweet'] = df['clean_tweet'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "# df.iloc[100][\"preprocessed_tweet\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['clean_tweet', 'preprocessed_tweet']].iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.2.2 <span style='color:#F1A424'>|</span> Lemmatization</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to lemmatise the tokens\n",
    "def lemmatise_tokens(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Lemmatise the tokens\n",
    "df['lemma_preprocessed_tweet'] = df['preprocessed_tweet'].apply(lambda x: lemmatise_tokens(x))\n",
    "\n",
    "# df.iloc[100][\"preprocessed_tweet\"][:20]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['clean_tweet', 'lemma_preprocessed_tweet']].iloc[260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to stem the tokens\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Stem the tokens\n",
    "df['stemma_preprocessed_tweet'] = df['preprocessed_tweet'].apply(lambda x: stem_tokens(x))\n",
    "\n",
    "# df.iloc[100][\"preprocessed_tweet\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['clean_tweet', 'stemma_preprocessed_tweet']].iloc[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3.2.3 <span style='color:#F1A424'>|</span> Calculating Sentiment Based on Polarity & Subjectivity</b>\n",
    "\n",
    "TextBlob is a Python library for processing textual data, including sentiment analysis. It uses natural language processing (NLP) and the Natural Language Toolkit (NLTK) to achieve its tasks. When a sentence is passed into TextBlob, it returns two outputs: polarity and subjectivity. The polarity score is a float within the range [-1, 1], where -1 indicates a negative sentiment and 1 indicates a positive sentiment. The subjectivity score is a float within the range, where 0 is very objective and 1 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Create a function to get the subjectivity\n",
    "def getSubjectivity(text):\n",
    "  return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "  return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Create two new columns 'Subjectivity' & 'Polarity'\n",
    "df['Subjectivity'] = df['clean_tweet'].apply(getSubjectivity)\n",
    "df['Polarity'] = df['clean_tweet'].apply(getPolarity)\n",
    "\n",
    "# Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
    "df[['clean_tweet','Subjectivity','Polarity']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the negative, positive and nuetral analysis\n",
    "def getAnalysis(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'\n",
    "  \n",
    "df['sentiment'] = df['Polarity'].apply(getAnalysis)\n",
    "\n",
    "# Show the dataframe\n",
    "df[['clean_tweet','Subjectivity','Polarity','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using VADER\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Create a function to get the sentiment scores\n",
    "# def sentiment_analyzer_scores(text):\n",
    "#     score = analyser.polarity_scores(text)\n",
    "#     return score\n",
    "\n",
    "# # Get the compound sentiment scores\n",
    "# df['compound_sentiment'] = df['clean_tweet'].apply(lambda x: sentiment_analyzer_scores(x)['compound'])\n",
    "\n",
    "# # Get the sentiment scores whereby there is positive, negative and neutral sentiment\n",
    "# df['sentiment'] = df['compound_sentiment'].apply(lambda x: 'positive' if x >= 0.05 else ('negative' if x <= -0.05 else 'neutral'))\n",
    "\n",
    "# df[['clean_tweet', 'compound_sentiment', 'sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_preprocessed_tweet'] = df['lemma_preprocessed_tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemma_preprocessed_tweet'] = df['stemma_preprocessed_tweet'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df['preprocessed_tweet'] = df['preprocessed_tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to csv using the name 'interim_data.csv' fo the data folder\n",
    "# df.to_csv('interim_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
